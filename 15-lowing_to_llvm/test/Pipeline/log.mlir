Args: /home/lfr/MLIR_Tutorial/build/15-lowing_to_llvm/src/Tools/NS-opt/NS-opt15 /home/lfr/MLIR_Tutorial/15-lowing_to_llvm/test/Pipeline/to_llvm_pipeline.mlir --north-star-basic-pipeline=DP_Nums=2 --mlir-print-ir-after-all --debug 
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemRefLayoutAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TypedAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ElementsAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistinctAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ResourceBlobManagerDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineBinaryOpExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineConstantExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineDimExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineMapStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::IntegerSetStorage)
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroOperands<mlir::TypeID::get() [with Trait = mlir::OpTrait::ZeroOperands]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneRegion<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneRegion]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroResults<mlir::TypeID::get() [with Trait = mlir::OpTrait::ZeroResults]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroSuccessors<mlir::TypeID::get() [with Trait = mlir::OpTrait::ZeroSuccessors]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoRegionArguments<mlir::TypeID::get() [with Trait = mlir::OpTrait::NoRegionArguments]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoTerminator<mlir::TypeID::get() [with Trait = mlir::OpTrait::NoTerminator]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlock<mlir::TypeID::get() [with Trait = mlir::OpTrait::SingleBlock]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OpInvariants<mlir::TypeID::get() [with Trait = mlir::OpTrait::OpInvariants]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::BytecodeOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AffineScope<mlir::TypeID::get() [with Trait = mlir::OpTrait::AffineScope]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsIsolatedFromAbove<mlir::TypeID::get() [with Trait = mlir::OpTrait::IsIsolatedFromAbove]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SymbolTable<mlir::TypeID::get() [with Trait = mlir::OpTrait::SymbolTable]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::SymbolOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::OpAsmOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface::Trait<mlir::TypeID::get() [with Trait = mlir::RegionKindInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasOnlyGraphRegion<mlir::TypeID::get() [with Trait = mlir::OpTrait::HasOnlyGraphRegion]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::ModuleOpGenericAdaptorBase::Properties)
Load new dialect in Context func
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolUserOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectInlinerInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConvertToLLVMPatternInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferizableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AutomaticAllocationScope<mlir::TypeID::get() [with Trait = mlir::OpTrait::AutomaticAllocationScope]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::CallableOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::FunctionOpInterface::Trait]::Empty>)
Load new dialect in Context north_star
Load new dialect in Context tensor
Load new dialect in Context affine
Load new dialect in Context arith
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithFastMathInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithIntegerOverflowFlagsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CastOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithRoundingModeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferDeallocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ValueBoundsOpInterface)
Load new dialect in Context ub
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ub::PoisonAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineMapAccessInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineDmaStartOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineDmaWaitOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LoopLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineReadOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineWriteOpInterface)
Load new dialect in Context complex
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ReifyRankedShapedTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedDimOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OffsetSizeAndStrideOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestinationStyleOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::FindPayloadReplacementOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetInsertionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetExtractionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TilingInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistributeParallelAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataParallelAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistributeParallelOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SupportedDataParallelismOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FusionRegionOpInterfaces)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroRegions<mlir::TypeID::get() [with Trait = mlir::OpTrait::ZeroRegions]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicOperands<mlir::TypeID::get() [with Trait = mlir::OpTrait::VariadicOperands]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::func::FuncOp>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::HasParent<mlir::func::FuncOp>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable::Trait<mlir::TypeID::get() [with Trait = mlir::ConditionallySpeculatable::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AlwaysSpeculatableImplTrait<mlir::TypeID::get() [with Trait = mlir::OpTrait::AlwaysSpeculatableImplTrait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::MemoryEffectOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::MemRefsNormalizable<mlir::TypeID::get() [with Trait = mlir::OpTrait::MemRefsNormalizable]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::RegionBranchTerminatorOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ReturnLike<mlir::TypeID::get() [with Trait = mlir::OpTrait::ReturnLike]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsTerminator<mlir::TypeID::get() [with Trait = mlir::OpTrait::IsTerminator]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutSpecInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneResult<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneResult]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::Type>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneTypedResult<mlir::Type>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneOperand<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneOperand]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistributeParallelOp::Trait<mlir::TypeID::get() [with Trait = mlir::DistributeParallelOp::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SupportedDataParallelismOp::Trait<mlir::TypeID::get() [with Trait = mlir::SupportedDataParallelismOp::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::OpToOpPassAdaptor)
Load new dialect in Context linalg
Load new dialect in Context math
Load new dialect in Context memref
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CopyOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableMemOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAccessorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableAllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ViewLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::AllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RuntimeVerifiableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::AggregatedOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::LinalgOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::ContractionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::ConvolutionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::FillOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::mesh::ShardingInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PartialReductionOpInterface)
run in MarkDistributeParallelParametersPass
root op: builtin.module
DPNums: 2
TPNums: 1
EPNums: 0
run out: MarkDistributeParallelParametersPass
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::PreservedAnalyses::AllAnalysesType)
// -----// IR Dump After MarkDistributeParallelParametersPass (mark-distribute-parallel-parameters) //----- //
module @NorthStar {
  func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
    %0 = "north_star.softmax"(%arg0) <{axis = 1 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.softmax"(%0) <{axis = 1 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0>
    return %1 : !north_star.ns_tensor<2x128xf32,0>
  }
}


run in ApplyDistributeTransformPass
root op: func.func
ImplicitTypeIDRegistry::lookupOrInsert(mlir::north_star::detail::BufferCastOpGenericAdaptorBase::Properties)
Apply DataParallelism to north_star.softmax
Apply DataParallelism to north_star.softmax
run out: ApplyDistributeTransformPass
// -----// IR Dump After ApplyDistributeTransformPass (apply-distribute-transform) //----- //
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %3 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  %4:2 = "north_star.buffer_cast"(%3) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %5 = "north_star.softmax"(%4#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %6 = "north_star.softmax"(%4#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %7 = "north_star.buffer_cast"(%5, %6) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %7 : !north_star.ns_tensor<2x128xf32,0>
}

ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicResults<mlir::TypeID::get() [with Trait = mlir::OpTrait::VariadicResults]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectFoldInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ConstantLike<mlir::TypeID::get() [with Trait = mlir::OpTrait::ConstantLike]::Empty>)

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5ffdbbbafbd0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6190) {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
"mlir::north_star::{anonymous}::BufferCastOpFold" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbdd290) {
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbf5bc0) {
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6230) {
  %3 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
"mlir::north_star::{anonymous}::BufferCastOpFold" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6bb0) {
  %4:2 = "north_star.buffer_cast"(%3) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
    ** Modified: 'north_star.softmax'(0x5ffdbbbaefb0)
    ** Modified: 'north_star.softmax'(0x5ffdbbbf6b10)
    ** Erase   : 'north_star.buffer_cast'(0x5ffdbbbf6bb0)
    ** Erase   : 'north_star.buffer_cast'(0x5ffdbbbf6230)
"mlir::north_star::{anonymous}::BufferCastOpFold" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %3 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %4 = "north_star.softmax"(%2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.buffer_cast"(%3, %4) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %5 : !north_star.ns_tensor<2x128xf32,0>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbf5bc0) {
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbdd290) {
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5ffdbbbafbd0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbaefb0) {
  %3 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbf6b10) {
  %4 = "north_star.softmax"(%2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6c50) {
  %5 = "north_star.buffer_cast"(%3, %4) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
"mlir::north_star::{anonymous}::BufferCastOpFold" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5ffdbbbaf6e0) {
  "func.return"(%5) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BranchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasRecursiveMemoryEffects<mlir::TypeID::get() [with Trait = mlir::OpTrait::HasRecursiveMemoryEffects]::Empty>)

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5ffdbbbafbd0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6190) {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
"mlir::north_star::{anonymous}::BufferCastOpFold" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbdd290) {
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbf5bc0) {
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbaefb0) {
  %3 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbf6b10) {
  %4 = "north_star.softmax"(%2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6c50) {
  %5 = "north_star.buffer_cast"(%3, %4) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
"mlir::north_star::{anonymous}::BufferCastOpFold" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5ffdbbbaf6e0) {
  "func.return"(%5) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module @NorthStar {
  func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
    %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
    %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %3 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %4 = "north_star.softmax"(%2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %5 = "north_star.buffer_cast"(%3, %4) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
    return %5 : !north_star.ns_tensor<2x128xf32,0>
  }
}


run in DeviceRegionFusionPass
root op: func.func

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5ffdbbbaf6e0) {
  "func.return"(%5) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6c50) {
  %5 = "north_star.buffer_cast"(%3, %4) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern {anonymous}::BufferCastOpDeviceRegionFusion : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpDeviceRegionFusion"
"{anonymous}::BufferCastOpDeviceRegionFusion" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbf6b10) {
  %4 = "north_star.softmax"(%2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbaefb0) {
  %3 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbf5bc0) {
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbdd290) {
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6190) {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern {anonymous}::BufferCastOpDeviceRegionFusion : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpDeviceRegionFusion"
ImplicitTypeIDRegistry::lookupOrInsert(mlir::north_star::detail::DeviceKernelOpGenericAdaptorBase::Properties)
    ** Insert  : 'north_star.device_kernel'(0x5ffdbbbf6ba0)
    ** Insert  : 'north_star.return'(0x5ffdbbbfe120)
    ** Modified: 'north_star.buffer_cast'(0x5ffdbbbf6c50)
    ** Insert  : 'north_star.device_kernel'(0x5ffdbbbfe230)
    ** Insert  : 'north_star.return'(0x5ffdbbbfe440)
    ** Modified: 'north_star.buffer_cast'(0x5ffdbbbf6c50)
"{anonymous}::BufferCastOpDeviceRegionFusion" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FusionRegionOpInterfaces::Trait<mlir::TypeID::get() [with Trait = mlir::FusionRegionOpInterfaces::Trait]::Empty>)
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %1 = "north_star.device_kernel"(%0#0) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,0>):
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::north_star::DeviceKernelOp>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::HasParent<mlir::north_star::DeviceKernelOp>::Impl]::Empty>)
    %8 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %9 = "north_star.softmax"(%8) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    north_star.return %9 : !north_star.ns_tensor<1x128xf32,0>
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "north_star.device_kernel"(%0#1) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %8 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %9 = "north_star.softmax"(%8) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    north_star.return %9 : !north_star.ns_tensor<1x128xf32,1>
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %3 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %4 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.softmax"(%3) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %6 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %7 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %7 : !north_star.ns_tensor<2x128xf32,0>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5ffdbbbfe440) {
  "north_star.return"(%9) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5ffdbbbfe230) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6c50) {
  %7 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern {anonymous}::BufferCastOpDeviceRegionFusion : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpDeviceRegionFusion"
"{anonymous}::BufferCastOpDeviceRegionFusion" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5ffdbbbfe120) {
  "north_star.return"(%11) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5ffdbbbf6ba0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//
** Erase   : 'north_star.softmax'(0x5ffdbbbf6b10)
** Erase   : 'north_star.softmax'(0x5ffdbbbaefb0)
** Erase   : 'north_star.softmax'(0x5ffdbbbf5bc0)
** Erase   : 'north_star.softmax'(0x5ffdbbbdd290)

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5ffdbbbaf6e0) {
  "func.return"(%3) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6c50) {
  %3 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern {anonymous}::BufferCastOpDeviceRegionFusion : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpDeviceRegionFusion"
"{anonymous}::BufferCastOpDeviceRegionFusion" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5ffdbbbfe440) {
  "north_star.return"(%5) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbfe3c0) {
  %5 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5ffdbbbfe230) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbfe330) {
  %4 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5ffdbbbfe120) {
  "north_star.return"(%7) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbfe060) {
  %7 = "north_star.softmax"(%6) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5ffdbbbf6ba0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbaf0a0) {
  %6 = "north_star.softmax"(%arg2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6190) {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern {anonymous}::BufferCastOpDeviceRegionFusion : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpDeviceRegionFusion"
"{anonymous}::BufferCastOpDeviceRegionFusion" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//
region has changed: true
run out: DeviceRegionFusionPass
// -----// IR Dump After DeviceRegionFusionPass (device-region-fusion) //----- //
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %1 = "north_star.device_kernel"(%0#0) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,0>):
    %4 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %5 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    north_star.return %5 : !north_star.ns_tensor<1x128xf32,0>
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "north_star.device_kernel"(%0#1) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %4 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %5 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    north_star.return %5 : !north_star.ns_tensor<1x128xf32,1>
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %3 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %3 : !north_star.ns_tensor<2x128xf32,0>
}

run in EliminateBufferCastPass

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5ffdbbbaf6e0) {
  "func.return"(%3) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6c50) {
  %3 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern {anonymous}::BufferCastOpToCommunicationPattern : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpToCommunicationPattern"
    ** Insert  : 'north_star.buffer'(0x5ffdbbbfc790)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ReifyRankedShapedTypeOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::ReifyRankedShapedTypeOpInterface::Trait]::Empty>)
    ** Insert  : 'tensor.empty'(0x5ffdbbbfe4d0)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::north_star::detail::TensorToNSTensorOpGenericAdaptorBase::Properties)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbdd290)
    ** Insert  : 'north_star.buffer'(0x5ffdbbbf5bc0)
    ** Insert  : 'north_star.gather'(0x5ffdbbbec3c0)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::north_star::detail::GetTensorOpGenericAdaptorBase::Properties)
    ** Insert  : 'north_star.get_tensor'(0x5ffdbbbaefb0)
    ** Replace : 'north_star.buffer_cast'(0x5ffdbbbf6c50)
    ** Modified: 'func.return'(0x5ffdbbbaf6e0)
"{anonymous}::BufferCastOpToCommunicationPattern" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %1 = "north_star.device_kernel"(%0#0) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,0>):
    %9 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %10 = "north_star.softmax"(%9) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    north_star.return %10 : !north_star.ns_tensor<1x128xf32,0>
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "north_star.device_kernel"(%0#1) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %9 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.softmax"(%9) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    north_star.return %10 : !north_star.ns_tensor<1x128xf32,1>
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %3 = "north_star.buffer"(%1, %2) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %4 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %6 = "north_star.buffer"(%5) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%3, %6) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %8 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %7 : !north_star.ns_tensor<2x128xf32,0>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5ffdbbbaf6e0) {
  "func.return"(%7) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5ffdbbbaefb0) {
  %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.gather'(0x5ffdbbbec3c0) {
  "north_star.gather"(%3, %6) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NOperands<2>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::NOperands<2>::Impl]::Empty>)
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5ffdbbbf5bc0) {
  %6 = "north_star.buffer"(%5) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbdd290) {
  %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5ffdbbbfe4d0) {
  %4 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5ffdbbbfc790) {
  %3 = "north_star.buffer"(%1, %2) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5ffdbbbfe440) {
  "north_star.return"(%10) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbfe3c0) {
  %10 = "north_star.softmax"(%9) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5ffdbbbfe230) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbfe330) {
  %9 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5ffdbbbfe120) {
  "north_star.return"(%12) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbfe060) {
  %12 = "north_star.softmax"(%11) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5ffdbbbf6ba0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbaf0a0) {
  %11 = "north_star.softmax"(%arg2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5ffdbbbafbd0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5ffdbbbf6190) {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern {anonymous}::BufferCastOpToCommunicationPattern : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpToCommunicationPattern"
    ** Insert  : 'north_star.buffer'(0x5ffdbbbf6b10)
    ** Insert  : 'tensor.empty'(0x5ffdbbbec490)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbfe540)
    ** Insert  : 'tensor.empty'(0x5ffdbbbec720)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbfba10)
    ** Insert  : 'north_star.buffer'(0x5ffdbbbfbaa0)
    ** Insert  : 'north_star.scatter'(0x5ffdbbbfbb40)
    ** Insert  : 'north_star.get_tensor'(0x5ffdbbbfbbf0)
    ** Insert  : 'north_star.get_tensor'(0x5ffdbbbfbc80)
    ** Replace : 'north_star.buffer_cast'(0x5ffdbbbf6190)
    ** Modified: 'north_star.device_kernel'(0x5ffdbbbf6ba0)
    ** Modified: 'north_star.device_kernel'(0x5ffdbbbfe230)
"{anonymous}::BufferCastOpToCommunicationPattern" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %1 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %3 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %7 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %8:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %9 = "north_star.device_kernel"(%6) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,0>):
    %17 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %18 = "north_star.softmax"(%17) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    north_star.return %18 : !north_star.ns_tensor<1x128xf32,0>
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %10 = "north_star.device_kernel"(%7) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %17 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %18 = "north_star.softmax"(%17) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    north_star.return %18 : !north_star.ns_tensor<1x128xf32,1>
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %11 = "north_star.buffer"(%9, %10) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %12 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %13 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %14 = "north_star.buffer"(%13) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%11, %14) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %15 = "north_star.get_tensor"(%14) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %16 = "north_star.buffer_cast"(%9, %10) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %15 : !north_star.ns_tensor<2x128xf32,0>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5ffdbbbfe230) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5ffdbbbf6ba0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5ffdbbbfbc80) {
  %7 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5ffdbbbfbbf0) {
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.scatter'(0x5ffdbbbfbb40) {
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5ffdbbbfbaa0) {
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbfba10) {
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5ffdbbbec720) {
  %3 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbfe540) {
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5ffdbbbec490) {
  %1 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5ffdbbbafbd0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5ffdbbbf6b10) {
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//
** Erase   : 'north_star.buffer_cast'(0x5ffdbbbf6c50)
** Erase   : 'north_star.buffer_cast'(0x5ffdbbbf6190)

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5ffdbbbaf6e0) {
  "func.return"(%14) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5ffdbbbaefb0) {
  %14 = "north_star.get_tensor"(%13) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.gather'(0x5ffdbbbec3c0) {
  "north_star.gather"(%10, %13) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5ffdbbbf5bc0) {
  %13 = "north_star.buffer"(%12) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbdd290) {
  %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5ffdbbbfe4d0) {
  %11 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5ffdbbbfc790) {
  %10 = "north_star.buffer"(%8, %9) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5ffdbbbfe440) {
  "north_star.return"(%16) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbfe3c0) {
  %16 = "north_star.softmax"(%15) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5ffdbbbfe230) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbfe330) {
  %15 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5ffdbbbfe120) {
  "north_star.return"(%18) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbfe060) {
  %18 = "north_star.softmax"(%17) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5ffdbbbf6ba0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5ffdbbbaf0a0) {
  %17 = "north_star.softmax"(%arg2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5ffdbbbfbc80) {
  %7 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5ffdbbbfbbf0) {
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.scatter'(0x5ffdbbbfbb40) {
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5ffdbbbfbaa0) {
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbfba10) {
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5ffdbbbec720) {
  %3 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbfe540) {
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5ffdbbbec490) {
  %1 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5ffdbbbafbd0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5ffdbbbf6b10) {
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//
run out: EliminateBufferCastPass
// -----// IR Dump After EliminateBufferCastPass (eliminate-buffercast) //----- //
module @NorthStar {
  func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
    %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    %1 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %3 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %7 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %8 = "north_star.device_kernel"(%6) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,0>):
      %15 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
      %16 = "north_star.softmax"(%15) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
      north_star.return %16 : !north_star.ns_tensor<1x128xf32,0>
    }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %9 = "north_star.device_kernel"(%7) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
      %15 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
      %16 = "north_star.softmax"(%15) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
      north_star.return %16 : !north_star.ns_tensor<1x128xf32,1>
    }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.buffer"(%8, %9) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    %11 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %13 = "north_star.buffer"(%12) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%10, %13) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %14 = "north_star.get_tensor"(%13) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    return %14 : !north_star.ns_tensor<2x128xf32,0>
  }
}


run in ConvertNorthStarToLinalgPass

//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x5ffdbbbabf60) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x5ffdbbbafbd0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5ffdbbbf6b10) {
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5ffdbbbec490) {
  %1 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbfe540) {
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5ffdbbbec720) {
  %3 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbfba10) {
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5ffdbbbfbaa0) {
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.scatter'(0x5ffdbbbfbb40) {
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5ffdbbbfbbf0) {
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5ffdbbbfbc80) {
  %7 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.device_kernel'(0x5ffdbbbf6ba0) {
  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.device_kernel -> ()' {
Trying to match "{anonymous}::DeviceKernelOpConvertPattern"
    ** Insert  : 'north_star.device_kernel'(0x5ffdbbbf6180)
    ** Insert Block into : 'north_star.device_kernel'(0x5ffdbbbf6180)
    ** Insert  : 'north_star.softmax'(0x5ffdbbbff2a0)
    ** Insert  : 'north_star.softmax'(0x5ffdbbc009c0)
    ** Insert  : 'north_star.return'(0x5ffdbbbfbf70)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc00c10)
    ** Replace : 'north_star.device_kernel'(0x5ffdbbbf6ba0)
"{anonymous}::DeviceKernelOpConvertPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5ffdbbbff2a0) {
      %22 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.softmax -> ()' {
Trying to match "{anonymous}::SoftmaxOpToLinalgPattern"
        ** Insert  : 'tensor.empty'(0x5ffdbbc03ce0)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::detail::SoftmaxOpGenericAdaptorBase::Properties)
        ** Insert  : 'linalg.softmax'(0x5ffdbbc03d50)
        ** Replace : 'north_star.softmax'(0x5ffdbbbff2a0)
"{anonymous}::SoftmaxOpToLinalgPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5ffdbbc03ce0) {
          %23 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5ffdbbc03d50) {
          %24 = "linalg.softmax"(%22, %23) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %1 = builtin.unrealized_conversion_cast %0 : !north_star.ns_tensor<1x128xf32,0> to tensor<1x128xf32>
  %2 = tensor.empty() : tensor<1x128xf32>
  %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %4 = "north_star.softmax"(%0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %5 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  north_star.return %5 : !north_star.ns_tensor<1x128xf32,0>
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.device_kernel'(0x5ffdbbbf6180) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5ffdbbbff2a0) {
      %25 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked 'ignored' during conversion
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5ffdbbc009c0) {
      %26 = "north_star.softmax"(%25) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.softmax -> ()' {
Trying to match "{anonymous}::SoftmaxOpToLinalgPattern"
        ** Insert  : 'tensor.empty'(0x5ffdbbc03ed0)
        ** Insert  : 'linalg.softmax'(0x5ffdbbc03f40)
        ** Replace : 'north_star.softmax'(0x5ffdbbc009c0)
"{anonymous}::SoftmaxOpToLinalgPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5ffdbbc03ed0) {
          %26 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5ffdbbc03f40) {
          %27 = "linalg.softmax"(%24, %26) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %1 = builtin.unrealized_conversion_cast %0 : !north_star.ns_tensor<1x128xf32,0> to tensor<1x128xf32>
  %2 = tensor.empty() : tensor<1x128xf32>
  %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %4 = "north_star.softmax"(%0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %5 = tensor.empty() : tensor<1x128xf32>
  %6 = linalg.softmax dimension(1) ins(%3 : tensor<1x128xf32>) outs(%5 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %7 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  north_star.return %7 : !north_star.ns_tensor<1x128xf32,0>
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.return'(0x5ffdbbbfbf70) {
      "north_star.return"(%28) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.return -> ()' {
Trying to match "{anonymous}::ReturnOpConvertPattern"
        ** Insert  : 'north_star.return'(0x5ffdbbbfca20)
        ** Replace : 'north_star.return'(0x5ffdbbbfbf70)
"{anonymous}::ReturnOpConvertPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'north_star.return'(0x5ffdbbbfca20) {
          "north_star.return"(%27) : (tensor<1x128xf32>) -> ()

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'north_star.return' op must be the last operation in the parent block
mlir-asm-printer: 'north_star.device_kernel' failed to verify and will be printed in generic form
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "builtin.unrealized_conversion_cast"(%1) : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %3 = "tensor.empty"() : () -> tensor<1x128xf32>
  %4 = "linalg.softmax"(%2, %3) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  %5 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %6 = "tensor.empty"() : () -> tensor<1x128xf32>
  %7 = "linalg.softmax"(%4, %6) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  %8 = "north_star.softmax"(%5) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  "north_star.return"(%7) : (tensor<1x128xf32>) -> ()
  "north_star.return"(%8) : (!north_star.ns_tensor<1x128xf32,0>) -> ()
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc00c10) {
      %21 = "north_star.tensor_to_ns_tensor"(%arg3) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'north_star.return' op must be the last operation in the parent block
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{function_type = (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0>, sym_name = "main"}> ({
^bb0(%arg0: !north_star.ns_tensor<2x128xf32,0>):
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %1 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %3 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %7 = "builtin.unrealized_conversion_cast"(%6) : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %8 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %9 = "north_star.device_kernel"(%7) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg3: tensor<1x128xf32>):
    %21 = "north_star.tensor_to_ns_tensor"(%arg3) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %22 = "builtin.unrealized_conversion_cast"(%21) : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %23 = "tensor.empty"() : () -> tensor<1x128xf32>
    %24 = "linalg.softmax"(%22, %23) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %25 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %26 = "tensor.empty"() : () -> tensor<1x128xf32>
    %27 = "linalg.softmax"(%24, %26) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %28 = "north_star.softmax"(%25) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    "north_star.return"(%27) : (tensor<1x128xf32>) -> ()
    "north_star.return"(%28) : (!north_star.ns_tensor<1x128xf32,0>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %10 = "north_star.device_kernel"(%6) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg2: !north_star.ns_tensor<1x128xf32,0>):
    %19 = "north_star.softmax"(%arg2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %20 = "north_star.softmax"(%19) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    "north_star.return"(%20) : (!north_star.ns_tensor<1x128xf32,0>) -> ()
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %11 = "north_star.device_kernel"(%8) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %17 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %18 = "north_star.softmax"(%17) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    "north_star.return"(%18) : (!north_star.ns_tensor<1x128xf32,1>) -> ()
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %12 = "north_star.buffer"(%10, %11) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %13 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
  %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %15 = "north_star.buffer"(%14) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%12, %15) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %16 = "north_star.get_tensor"(%15) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  "func.return"(%16) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
}) {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} : () -> ()


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.softmax'(0x5ffdbbbaf0a0) {
  %19 = "north_star.softmax"(%arg2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.softmax'(0x5ffdbbbfe060) {
  %20 = "north_star.softmax"(%19) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.return'(0x5ffdbbbfe120) {
  "north_star.return"(%20) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.device_kernel'(0x5ffdbbbfe230) {
  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.device_kernel -> ()' {
Trying to match "{anonymous}::DeviceKernelOpConvertPattern"
    ** Insert  : 'north_star.device_kernel'(0x5ffdbbc02240)
    ** Insert Block into : 'north_star.device_kernel'(0x5ffdbbc02240)
    ** Insert  : 'north_star.softmax'(0x5ffdbbc00cd0)
    ** Insert  : 'north_star.softmax'(0x5ffdbbc02320)
    ** Insert  : 'north_star.return'(0x5ffdbbc01f70)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc02600)
    ** Replace : 'north_star.device_kernel'(0x5ffdbbbfe230)
"{anonymous}::DeviceKernelOpConvertPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5ffdbbc00cd0) {
      %22 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.softmax -> ()' {
Trying to match "{anonymous}::SoftmaxOpToLinalgPattern"
        ** Insert  : 'tensor.empty'(0x5ffdbbbc4940)
        ** Insert  : 'linalg.softmax'(0x5ffdbbc03810)
        ** Replace : 'north_star.softmax'(0x5ffdbbc00cd0)
"{anonymous}::SoftmaxOpToLinalgPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5ffdbbbc4940) {
          %23 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5ffdbbc03810) {
          %24 = "linalg.softmax"(%22, %23) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %1 = builtin.unrealized_conversion_cast %0 : !north_star.ns_tensor<1x128xf32,1> to tensor<1x128xf32>
  %2 = tensor.empty() : tensor<1x128xf32>
  %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %4 = "north_star.softmax"(%0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  north_star.return %5 : !north_star.ns_tensor<1x128xf32,1>
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.device_kernel'(0x5ffdbbc02240) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5ffdbbc00cd0) {
      %25 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked 'ignored' during conversion
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5ffdbbc02320) {
      %26 = "north_star.softmax"(%25) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.softmax -> ()' {
Trying to match "{anonymous}::SoftmaxOpToLinalgPattern"
        ** Insert  : 'tensor.empty'(0x5ffdbbc03950)
        ** Insert  : 'linalg.softmax'(0x5ffdbbc039c0)
        ** Replace : 'north_star.softmax'(0x5ffdbbc02320)
"{anonymous}::SoftmaxOpToLinalgPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5ffdbbc03950) {
          %26 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5ffdbbc039c0) {
          %27 = "linalg.softmax"(%24, %26) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %1 = builtin.unrealized_conversion_cast %0 : !north_star.ns_tensor<1x128xf32,1> to tensor<1x128xf32>
  %2 = tensor.empty() : tensor<1x128xf32>
  %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %4 = "north_star.softmax"(%0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = tensor.empty() : tensor<1x128xf32>
  %6 = linalg.softmax dimension(1) ins(%3 : tensor<1x128xf32>) outs(%5 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %7 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  north_star.return %7 : !north_star.ns_tensor<1x128xf32,1>
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.return'(0x5ffdbbc01f70) {
      "north_star.return"(%28) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.return -> ()' {
Trying to match "{anonymous}::ReturnOpConvertPattern"
        ** Insert  : 'north_star.return'(0x5ffdbbc03b30)
        ** Replace : 'north_star.return'(0x5ffdbbc01f70)
"{anonymous}::ReturnOpConvertPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'north_star.return'(0x5ffdbbc03b30) {
          "north_star.return"(%27) : (tensor<1x128xf32>) -> ()

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'north_star.return' op must be the last operation in the parent block
mlir-asm-printer: 'north_star.device_kernel' failed to verify and will be printed in generic form
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %2 = "builtin.unrealized_conversion_cast"(%1) : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  %3 = "tensor.empty"() : () -> tensor<1x128xf32>
  %4 = "linalg.softmax"(%2, %3) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  %5 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %6 = "tensor.empty"() : () -> tensor<1x128xf32>
  %7 = "linalg.softmax"(%4, %6) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  %8 = "north_star.softmax"(%5) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  "north_star.return"(%7) : (tensor<1x128xf32>) -> ()
  "north_star.return"(%8) : (!north_star.ns_tensor<1x128xf32,1>) -> ()
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc02600) {
      %21 = "north_star.tensor_to_ns_tensor"(%arg2) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'north_star.return' op must be the last operation in the parent block
'north_star.return' op must be the last operation in the parent block
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{function_type = (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0>, sym_name = "main"}> ({
^bb0(%arg0: !north_star.ns_tensor<2x128xf32,0>):
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %1 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %3 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %7 = "builtin.unrealized_conversion_cast"(%6) : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %8 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %9 = "builtin.unrealized_conversion_cast"(%8) : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  %10 = "north_star.device_kernel"(%7) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg4: tensor<1x128xf32>):
    %31 = "north_star.tensor_to_ns_tensor"(%arg4) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %32 = "builtin.unrealized_conversion_cast"(%31) : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %33 = "tensor.empty"() : () -> tensor<1x128xf32>
    %34 = "linalg.softmax"(%32, %33) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %35 = "north_star.softmax"(%31) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %36 = "tensor.empty"() : () -> tensor<1x128xf32>
    %37 = "linalg.softmax"(%34, %36) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %38 = "north_star.softmax"(%35) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    "north_star.return"(%37) : (tensor<1x128xf32>) -> ()
    "north_star.return"(%38) : (!north_star.ns_tensor<1x128xf32,0>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %11 = "north_star.device_kernel"(%6) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg3: !north_star.ns_tensor<1x128xf32,0>):
    %29 = "north_star.softmax"(%arg3) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %30 = "north_star.softmax"(%29) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    "north_star.return"(%30) : (!north_star.ns_tensor<1x128xf32,0>) -> ()
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %12 = "north_star.device_kernel"(%9) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg2: tensor<1x128xf32>):
    %21 = "north_star.tensor_to_ns_tensor"(%arg2) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %22 = "builtin.unrealized_conversion_cast"(%21) : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %23 = "tensor.empty"() : () -> tensor<1x128xf32>
    %24 = "linalg.softmax"(%22, %23) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %25 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %26 = "tensor.empty"() : () -> tensor<1x128xf32>
    %27 = "linalg.softmax"(%24, %26) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %28 = "north_star.softmax"(%25) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    "north_star.return"(%27) : (tensor<1x128xf32>) -> ()
    "north_star.return"(%28) : (!north_star.ns_tensor<1x128xf32,1>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %13 = "north_star.device_kernel"(%8) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %19 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %20 = "north_star.softmax"(%19) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    "north_star.return"(%20) : (!north_star.ns_tensor<1x128xf32,1>) -> ()
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %14 = "north_star.buffer"(%11, %13) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %15 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
  %16 = "north_star.tensor_to_ns_tensor"(%15) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %17 = "north_star.buffer"(%16) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%14, %17) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %18 = "north_star.get_tensor"(%17) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  "func.return"(%18) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
}) {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} : () -> ()


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.softmax'(0x5ffdbbbfe330) {
  %19 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.softmax'(0x5ffdbbbfe3c0) {
  %20 = "north_star.softmax"(%19) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.return'(0x5ffdbbbfe440) {
  "north_star.return"(%20) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5ffdbbbfc790) {
  %14 = "north_star.buffer"(%11, %13) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5ffdbbbfe4d0) {
  %15 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbdd290) {
  %16 = "north_star.tensor_to_ns_tensor"(%15) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5ffdbbbf5bc0) {
  %17 = "north_star.buffer"(%16) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.gather'(0x5ffdbbbec3c0) {
  "north_star.gather"(%14, %17) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5ffdbbbaefb0) {
  %18 = "north_star.get_tensor"(%17) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x5ffdbbbaf6e0) {
  "func.return"(%18) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::north_star::detail::NSTensorToTensorOpGenericAdaptorBase::Properties)
** Insert  : 'north_star.ns_tensor_to_tensor'(0x5ffdbbc028b0)
** Insert  : 'north_star.ns_tensor_to_tensor'(0x5ffdbbc027d0)
** Insert  : 'north_star.ns_tensor_to_tensor'(0x5ffdbbc02dc0)
** Insert  : 'north_star.ns_tensor_to_tensor'(0x5ffdbbc02e50)
** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc02f10)
** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc02fd0)
run out: ConvertNorthStarToLinalgPass
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestinationStyleOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::DestinationStyleOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::AggregatedOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::linalg::AggregatedOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TilingInterface::Trait<mlir::TypeID::get() [with Trait = mlir::TilingInterface::Trait]::Empty>)
// -----// IR Dump After ConvertNorthStarToLinalgPass (convert-north-satr-to-linalg) //----- //
module @NorthStar {
  func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
    %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    %1 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %3 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %7 = "north_star.ns_tensor_to_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %8 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %9 = "north_star.ns_tensor_to_tensor"(%8) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %10 = "north_star.device_kernel"(%7) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg1: tensor<1x128xf32>):
      %19 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
      %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
      %21 = tensor.empty() : tensor<1x128xf32>
      %22 = linalg.softmax dimension(1) ins(%20 : tensor<1x128xf32>) outs(%21 : tensor<1x128xf32>) -> tensor<1x128xf32>
      %23 = tensor.empty() : tensor<1x128xf32>
      %24 = linalg.softmax dimension(1) ins(%22 : tensor<1x128xf32>) outs(%23 : tensor<1x128xf32>) -> tensor<1x128xf32>
      north_star.return %24 : tensor<1x128xf32>
    }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %11 = "north_star.tensor_to_ns_tensor"(%10) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %12 = "north_star.device_kernel"(%9) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg1: tensor<1x128xf32>):
      %19 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
      %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
      %21 = tensor.empty() : tensor<1x128xf32>
      %22 = linalg.softmax dimension(1) ins(%20 : tensor<1x128xf32>) outs(%21 : tensor<1x128xf32>) -> tensor<1x128xf32>
      %23 = tensor.empty() : tensor<1x128xf32>
      %24 = linalg.softmax dimension(1) ins(%22 : tensor<1x128xf32>) outs(%23 : tensor<1x128xf32>) -> tensor<1x128xf32>
      north_star.return %24 : tensor<1x128xf32>
    }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %13 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %14 = "north_star.buffer"(%11, %13) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    %15 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %16 = "north_star.tensor_to_ns_tensor"(%15) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %17 = "north_star.buffer"(%16) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%14, %17) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %18 = "north_star.get_tensor"(%17) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    return %18 : !north_star.ns_tensor<2x128xf32,0>
  }
}


run in ConvertNorthStarToFuncPass

//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x5ffdbbbabf60) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x5ffdbbbafbd0) {
  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'func.func -> ()' {
Trying to match "{anonymous}::FuncFuncOpRerewriterPattern"
    ** Insert  : 'func.func'(0x5ffdbbc00b40)
    ** Insert Block into : 'func.func'(0x5ffdbbc00b40)
    ** Insert  : 'north_star.buffer'(0x5ffdbbc03ad0)
    ** Insert  : 'tensor.empty'(0x5ffdbbc03ff0)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbc49b0)
    ** Insert  : 'tensor.empty'(0x5ffdbbc04090)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc038c0)
    ** Insert  : 'north_star.buffer'(0x5ffdbbbf77d0)
    ** Insert  : 'north_star.scatter'(0x5ffdbbc02930)
    ** Insert  : 'north_star.get_tensor'(0x5ffdbbc03e00)
    ** Insert  : 'north_star.ns_tensor_to_tensor'(0x5ffdbbbfe330)
    ** Insert  : 'north_star.get_tensor'(0x5ffdbbbfe3a0)
    ** Insert  : 'north_star.ns_tensor_to_tensor'(0x5ffdbbbfc3f0)
    ** Insert  : 'north_star.device_kernel'(0x5ffdbbc00ac0)
    ** Insert Block into : 'north_star.device_kernel'(0x5ffdbbc00ac0)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbffc10)
    ** Insert  : 'north_star.ns_tensor_to_tensor'(0x5ffdbbbffc80)
    ** Insert  : 'tensor.empty'(0x5ffdbbbffcf0)
    ** Insert  : 'linalg.softmax'(0x5ffdbbbffd60)
    ** Insert  : 'tensor.empty'(0x5ffdbbbffdd0)
    ** Insert  : 'linalg.softmax'(0x5ffdbbbffe40)
    ** Insert  : 'north_star.return'(0x5ffdbbbffea0)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbfc460)
    ** Insert  : 'north_star.device_kernel'(0x5ffdbbc00a30)
    ** Insert Block into : 'north_star.device_kernel'(0x5ffdbbc00a30)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc04fa0)
    ** Insert  : 'north_star.ns_tensor_to_tensor'(0x5ffdbbc05010)
    ** Insert  : 'tensor.empty'(0x5ffdbbc05080)
    ** Insert  : 'linalg.softmax'(0x5ffdbbc050f0)
    ** Insert  : 'tensor.empty'(0x5ffdbbc05160)
    ** Insert  : 'linalg.softmax'(0x5ffdbbc051d0)
    ** Insert  : 'north_star.return'(0x5ffdbbc05230)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc03690)
    ** Insert  : 'north_star.buffer'(0x5ffdbbc03700)
    ** Insert  : 'tensor.empty'(0x5ffdbbc03770)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc02390)
    ** Insert  : 'north_star.buffer'(0x5ffdbbc02400)
    ** Insert  : 'north_star.gather'(0x5ffdbbc02990)
    ** Insert  : 'north_star.get_tensor'(0x5ffdbbc02470)
    ** Insert  : 'func.return'(0x5ffdbbc03e60)
    ** Insert Block into : 'func.func'(0x5ffdbbc00b40)
'func.func' op entry block must have 1 arguments to match function signature
mlir-asm-printer: 'builtin.module' failed to verify and will be printed in generic form
^bb0:
  %31 = "builtin.unrealized_conversion_cast"() : () -> !north_star.ns_tensor<2x128xf32,0>
  %32 = "north_star.buffer"(<<UNKNOWN SSA VALUE>>) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %33 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
  %34 = "north_star.tensor_to_ns_tensor"(%33) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %35 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
  %36 = "north_star.tensor_to_ns_tensor"(%35) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %37 = "north_star.buffer"(%34, %36) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%32, %37) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %38 = "north_star.get_tensor"(%37) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %39 = "north_star.ns_tensor_to_tensor"(%38) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %40 = "north_star.get_tensor"(%37) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %41 = "north_star.ns_tensor_to_tensor"(%40) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  %42 = "north_star.device_kernel"(%39) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg4: tensor<1x128xf32>):
    %57 = "north_star.tensor_to_ns_tensor"(%arg4) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %58 = "north_star.ns_tensor_to_tensor"(%57) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %59 = "tensor.empty"() : () -> tensor<1x128xf32>
    %60 = "linalg.softmax"(%58, %59) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %61 = "tensor.empty"() : () -> tensor<1x128xf32>
    %62 = "linalg.softmax"(%60, %61) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%62) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %43 = "north_star.tensor_to_ns_tensor"(%42) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %44 = "north_star.device_kernel"(%41) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg3: tensor<1x128xf32>):
    %51 = "north_star.tensor_to_ns_tensor"(%arg3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %52 = "north_star.ns_tensor_to_tensor"(%51) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %53 = "tensor.empty"() : () -> tensor<1x128xf32>
    %54 = "linalg.softmax"(%52, %53) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %55 = "tensor.empty"() : () -> tensor<1x128xf32>
    %56 = "linalg.softmax"(%54, %55) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%56) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %45 = "north_star.tensor_to_ns_tensor"(%44) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %46 = "north_star.buffer"(%43, %45) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %47 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
  %48 = "north_star.tensor_to_ns_tensor"(%47) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %49 = "north_star.buffer"(%48) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%46, %49) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %50 = "north_star.get_tensor"(%49) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  "func.return"(%50) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
'func.func' op entry block must have 1 arguments to match function signature
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{function_type = (tensor<2x128xf32>) -> tensor<2x128xf32>, sym_name = "main"}> ({
  %0 = "builtin.unrealized_conversion_cast"() : () -> !north_star.ns_tensor<2x128xf32,0>
  %1 = "north_star.buffer"(<<UNKNOWN SSA VALUE>>) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %2 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
  %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %4 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
  %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  %11 = "north_star.device_kernel"(%8) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: tensor<1x128xf32>):
    %26 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %27 = "north_star.ns_tensor_to_tensor"(%26) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %28 = "tensor.empty"() : () -> tensor<1x128xf32>
    %29 = "linalg.softmax"(%27, %28) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %30 = "tensor.empty"() : () -> tensor<1x128xf32>
    %31 = "linalg.softmax"(%29, %30) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%31) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %13 = "north_star.device_kernel"(%10) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg0: tensor<1x128xf32>):
    %20 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %21 = "north_star.ns_tensor_to_tensor"(%20) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %22 = "tensor.empty"() : () -> tensor<1x128xf32>
    %23 = "linalg.softmax"(%21, %22) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %24 = "tensor.empty"() : () -> tensor<1x128xf32>
    %25 = "linalg.softmax"(%23, %24) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%25) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %16 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
  %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  "func.return"(%19) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
}) : () -> ()
    ** Replace : 'func.func'(0x5ffdbbbafbd0)
"{anonymous}::FuncFuncOpRerewriterPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'func.func'(0x5ffdbbc00b40) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.buffer'(0x5ffdbbc03ad0) {
      %32 = "north_star.buffer"(<<UNKNOWN SSA VALUE>>) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5ffdbbc03ff0) {
      %33 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbc49b0) {
      %34 = "north_star.tensor_to_ns_tensor"(%33) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5ffdbbc04090) {
      %35 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc038c0) {
      %36 = "north_star.tensor_to_ns_tensor"(%35) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.buffer'(0x5ffdbbbf77d0) {
      %37 = "north_star.buffer"(%34, %36) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.scatter'(0x5ffdbbc02930) {
      "north_star.scatter"(%32, %37) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.get_tensor'(0x5ffdbbc03e00) {
      %38 = "north_star.get_tensor"(%37) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5ffdbbbfe330) {
      %39 = "north_star.ns_tensor_to_tensor"(%38) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.get_tensor'(0x5ffdbbbfe3a0) {
      %40 = "north_star.get_tensor"(%37) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5ffdbbbfc3f0) {
      %41 = "north_star.ns_tensor_to_tensor"(%40) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.device_kernel'(0x5ffdbbc00ac0) {
      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.device_kernel -> ()' {
Trying to match "{anonymous}::DeviceKernelOpToFuncPattern"
        ** Insert  : 'func.call'(0x5ffdbbc09320)
        ** Insert Block into : 'func.func'(0x5ffdbbc05ff0)
        ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc093b0)
        ** Insert  : 'north_star.ns_tensor_to_tensor'(0x5ffdbbc09420)
        ** Insert  : 'tensor.empty'(0x5ffdbbc09490)
        ** Insert  : 'linalg.softmax'(0x5ffdbbc09500)
        ** Insert  : 'tensor.empty'(0x5ffdbbc09570)
        ** Insert  : 'linalg.softmax'(0x5ffdbbc07df0)
        ** Insert  : 'north_star.return'(0x5ffdbbc07e50)
        ** Replace : 'north_star.device_kernel'(0x5ffdbbc00ac0)
"{anonymous}::DeviceKernelOpToFuncPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'func.func'(0x5ffdbbc05ff0) {
        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'func.call'(0x5ffdbbc09320) {
          %48 = "func.call"(%45) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc093b0) {
          %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5ffdbbc09420) {
          %1 = "north_star.ns_tensor_to_tensor"(%0) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5ffdbbc09490) {
          %2 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5ffdbbc09500) {
          %3 = "linalg.softmax"(%1, %2) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5ffdbbc09570) {
          %4 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5ffdbbc07df0) {
          %5 = "linalg.softmax"(%3, %4) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'north_star.return'(0x5ffdbbc07e50) {
          "north_star.return"(%5) : (tensor<1x128xf32>) -> ()

          * Fold {
          } -> FAILURE : unable to fold

          * Pattern : 'north_star.return -> ()' {
Trying to match "{anonymous}::ReturnOpToFuncPattern"
            ** Insert  : 'func.return'(0x5ffdbbbfc700)
            ** Replace : 'north_star.return'(0x5ffdbbc07e50)
"{anonymous}::ReturnOpToFuncPattern" result 1

            //===-------------------------------------------===//
            Legalizing operation : 'func.return'(0x5ffdbbbfc700) {
              "func.return"(%5) : (tensor<1x128xf32>) -> ()

            } -> SUCCESS : operation marked legal by the target
            //===-------------------------------------------===//
          } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'func.return' op must be the last operation in the parent block
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{function_type = (tensor<1x128xf32>) -> tensor<1x128xf32>, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %1 = "north_star.ns_tensor_to_tensor"(%0) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %2 = "tensor.empty"() : () -> tensor<1x128xf32>
  %3 = "linalg.softmax"(%1, %2) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  %4 = "tensor.empty"() : () -> tensor<1x128xf32>
  %5 = "linalg.softmax"(%3, %4) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  "func.return"(%5) : (tensor<1x128xf32>) -> ()
  "north_star.return"(%5) : (tensor<1x128xf32>) -> ()
}) {device_kernel} : () -> ()


        } -> SUCCESS
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'func.func' op entry block must have 1 arguments to match function signature
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{function_type = (tensor<2x128xf32>) -> tensor<2x128xf32>, sym_name = "main"}> ({
  %0 = "builtin.unrealized_conversion_cast"() : () -> !north_star.ns_tensor<2x128xf32,0>
  %1 = "north_star.buffer"(<<UNKNOWN SSA VALUE>>) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %2 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
  %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %4 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
  %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  %11 = "func.call"(%8) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %12 = "north_star.device_kernel"(%8) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: tensor<1x128xf32>):
    %27 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %28 = "north_star.ns_tensor_to_tensor"(%27) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %29 = "tensor.empty"() : () -> tensor<1x128xf32>
    %30 = "linalg.softmax"(%28, %29) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = "tensor.empty"() : () -> tensor<1x128xf32>
    %32 = "linalg.softmax"(%30, %31) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%32) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %13 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %14 = "north_star.device_kernel"(%10) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg0: tensor<1x128xf32>):
    %21 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %22 = "north_star.ns_tensor_to_tensor"(%21) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %23 = "tensor.empty"() : () -> tensor<1x128xf32>
    %24 = "linalg.softmax"(%22, %23) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %25 = "tensor.empty"() : () -> tensor<1x128xf32>
    %26 = "linalg.softmax"(%24, %25) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%26) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %15 = "north_star.tensor_to_ns_tensor"(%14) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %16 = "north_star.buffer"(%13, %15) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %17 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
  %18 = "north_star.tensor_to_ns_tensor"(%17) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %19 = "north_star.buffer"(%18) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%16, %19) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %20 = "north_star.get_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  "func.return"(%20) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
}) : () -> ()


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbffc10) {
      %64 = "north_star.tensor_to_ns_tensor"(%arg5) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5ffdbbbffc80) {
      %65 = "north_star.ns_tensor_to_tensor"(%64) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5ffdbbbffcf0) {
      %66 = "tensor.empty"() : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.softmax'(0x5ffdbbbffd60) {
      %67 = "linalg.softmax"(%65, %66) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5ffdbbbffdd0) {
      %68 = "tensor.empty"() : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.softmax'(0x5ffdbbbffe40) {
      %69 = "linalg.softmax"(%67, %68) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.return'(0x5ffdbbbffea0) {
      "north_star.return"(%69) : (tensor<1x128xf32>) -> ()

    } -> SUCCESS : operation marked 'ignored' during conversion
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbbfc460) {
      %50 = "north_star.tensor_to_ns_tensor"(%49) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.device_kernel'(0x5ffdbbc00a30) {
      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.device_kernel -> ()' {
Trying to match "{anonymous}::DeviceKernelOpToFuncPattern"
        ** Insert  : 'func.call'(0x5ffdbbc0b7a0)
        ** Replace : 'north_star.device_kernel'(0x5ffdbbc00a30)
"{anonymous}::DeviceKernelOpToFuncPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'func.call'(0x5ffdbbc0b7a0) {
          %51 = "func.call"(%47) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'func.func' op entry block must have 1 arguments to match function signature
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{function_type = (tensor<2x128xf32>) -> tensor<2x128xf32>, sym_name = "main"}> ({
  %0 = "builtin.unrealized_conversion_cast"() : () -> !north_star.ns_tensor<2x128xf32,0>
  %1 = "north_star.buffer"(<<UNKNOWN SSA VALUE>>) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %2 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
  %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %4 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
  %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  %11 = "func.call"(%8) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %12 = "north_star.device_kernel"(%8) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: tensor<1x128xf32>):
    %28 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %29 = "north_star.ns_tensor_to_tensor"(%28) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %30 = "tensor.empty"() : () -> tensor<1x128xf32>
    %31 = "linalg.softmax"(%29, %30) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %32 = "tensor.empty"() : () -> tensor<1x128xf32>
    %33 = "linalg.softmax"(%31, %32) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%33) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %13 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %14 = "func.call"(%10) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %15 = "north_star.device_kernel"(%10) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg0: tensor<1x128xf32>):
    %22 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %23 = "north_star.ns_tensor_to_tensor"(%22) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %24 = "tensor.empty"() : () -> tensor<1x128xf32>
    %25 = "linalg.softmax"(%23, %24) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %26 = "tensor.empty"() : () -> tensor<1x128xf32>
    %27 = "linalg.softmax"(%25, %26) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%27) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %16 = "north_star.tensor_to_ns_tensor"(%15) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %17 = "north_star.buffer"(%13, %16) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %18 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
  %19 = "north_star.tensor_to_ns_tensor"(%18) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %20 = "north_star.buffer"(%19) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%17, %20) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %21 = "north_star.get_tensor"(%20) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  "func.return"(%21) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
}) : () -> ()


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc04fa0) {
      %59 = "north_star.tensor_to_ns_tensor"(%arg4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5ffdbbc05010) {
      %60 = "north_star.ns_tensor_to_tensor"(%59) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5ffdbbc05080) {
      %61 = "tensor.empty"() : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.softmax'(0x5ffdbbc050f0) {
      %62 = "linalg.softmax"(%60, %61) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5ffdbbc05160) {
      %63 = "tensor.empty"() : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.softmax'(0x5ffdbbc051d0) {
      %64 = "linalg.softmax"(%62, %63) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.return'(0x5ffdbbc05230) {
      "north_star.return"(%64) : (tensor<1x128xf32>) -> ()

    } -> SUCCESS : operation marked 'ignored' during conversion
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc03690) {
      %53 = "north_star.tensor_to_ns_tensor"(%52) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.buffer'(0x5ffdbbc03700) {
      %54 = "north_star.buffer"(%50, %53) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5ffdbbc03770) {
      %55 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5ffdbbc02390) {
      %56 = "north_star.tensor_to_ns_tensor"(%55) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.buffer'(0x5ffdbbc02400) {
      %57 = "north_star.buffer"(%56) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.gather'(0x5ffdbbc02990) {
      "north_star.gather"(%54, %57) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.get_tensor'(0x5ffdbbc02470) {
      %58 = "north_star.get_tensor"(%57) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.return'(0x5ffdbbc03e60) {
      "func.return"(%58) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'func.return -> ()' {
Trying to match "{anonymous}::FuncReturnOpRerewriterPattern"
        ** Insert  : 'north_star.ns_tensor_to_tensor'(0x5ffdbbc0cd10)
NS-opt15: /home/lfr/MLIR_Tutorial/third_party/llvm-project/llvm/include/llvm/Support/Casting.h:566: decltype(auto) llvm::cast(const From&) [with To = mlir::detail::TypedValue<mlir::RankedTensorType>; From = mlir::OpResult]: Assertion `isa<To>(Val) && "cast<Ty>() argument of incompatible type!"' failed.
PLEASE submit a bug report to https://github.com/llvm/llvm-project/issues/ and include the crash backtrace.
Stack dump:
0.	Program arguments: /home/lfr/MLIR_Tutorial/build/15-lowing_to_llvm/src/Tools/NS-opt/NS-opt15 /home/lfr/MLIR_Tutorial/15-lowing_to_llvm/test/Pipeline/to_llvm_pipeline.mlir --north-star-basic-pipeline=DP_Nums=2 --mlir-print-ir-after-all --debug
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
0  NS-opt15  0x00005ffdab0b1738 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 82
1  NS-opt15  0x00005ffdab0b1b4e
2  NS-opt15  0x00005ffdab0af1aa llvm::sys::RunSignalHandlers() + 159
3  NS-opt15  0x00005ffdab0b0ff2
4  libc.so.6 0x00007fee83a42520
5  libc.so.6 0x00007fee83a969fc pthread_kill + 300
6  libc.so.6 0x00007fee83a42476 raise + 22
7  libc.so.6 0x00007fee83a287f3 abort + 211
8  libc.so.6 0x00007fee83a2871b
9  libc.so.6 0x00007fee83a39e96
10 NS-opt15  0x00005ffda8b3bbc0
11 NS-opt15  0x00005ffda8b4cae4 mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl<mlir::north_star::NSTensorToTensorOp>::getResult() + 72
12 NS-opt15  0x00005ffda8b4c67c mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl<mlir::north_star::NSTensorToTensorOp>::operator mlir::detail::TypedValue<mlir::RankedTensorType>() + 28
13 NS-opt15  0x00005ffda8b4e4eb
14 NS-opt15  0x00005ffda8b57507 mlir::OpConversionPattern<mlir::func::ReturnOp>::matchAndRewrite(mlir::func::ReturnOp, mlir::func::ReturnOpAdaptor, mlir::ConversionPatternRewriter&) const + 137
15 NS-opt15  0x00005ffda8b5745f mlir::OpConversionPattern<mlir::func::ReturnOp>::matchAndRewrite(mlir::Operation*, llvm::ArrayRef<mlir::Value>, mlir::ConversionPatternRewriter&) const + 191
16 NS-opt15  0x00005ffdaa877ba0 mlir::ConversionPattern::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&) const + 412
17 NS-opt15  0x00005ffdaa8f25d3
18 NS-opt15  0x00005ffdaa8f2f6e
19 NS-opt15  0x00005ffda876aba2
20 NS-opt15  0x00005ffdaa8f61c9
21 NS-opt15  0x00005ffdaa8f2d35 mlir::PatternApplicator::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&, llvm::function_ref<bool (mlir::Pattern const&)>, llvm::function_ref<void (mlir::Pattern const&)>, llvm::function_ref<llvm::LogicalResult (mlir::Pattern const&)>) + 1447
22 NS-opt15  0x00005ffdaa878da7
23 NS-opt15  0x00005ffdaa8781dd
24 NS-opt15  0x00005ffdaa879881
25 NS-opt15  0x00005ffdaa87920b
26 NS-opt15  0x00005ffdaa878b4c
27 NS-opt15  0x00005ffdaa8855d8
28 NS-opt15  0x00005ffdaa8f611b
29 NS-opt15  0x00005ffdaa8f26c8
30 NS-opt15  0x00005ffdaa8f2f6e
31 NS-opt15  0x00005ffda876aba2
32 NS-opt15  0x00005ffdaa8f61c9
33 NS-opt15  0x00005ffdaa8f2d35 mlir::PatternApplicator::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&, llvm::function_ref<bool (mlir::Pattern const&)>, llvm::function_ref<void (mlir::Pattern const&)>, llvm::function_ref<llvm::LogicalResult (mlir::Pattern const&)>) + 1447
34 NS-opt15  0x00005ffdaa878da7
35 NS-opt15  0x00005ffdaa8781dd
36 NS-opt15  0x00005ffdaa87a9ca mlir::OperationConverter::convert(mlir::ConversionPatternRewriter&, mlir::Operation*) + 88
37 NS-opt15  0x00005ffdaa87aea2 mlir::OperationConverter::convertOperations(llvm::ArrayRef<mlir::Operation*>) + 488
38 NS-opt15  0x00005ffdaa880ae8 mlir::applyPartialConversion(llvm::ArrayRef<mlir::Operation*>, mlir::ConversionTarget const&, mlir::FrozenRewritePatternSet const&, mlir::ConversionConfig) + 145
39 NS-opt15  0x00005ffdaa880b81 mlir::applyPartialConversion(mlir::Operation*, mlir::ConversionTarget const&, mlir::FrozenRewritePatternSet const&, mlir::ConversionConfig) + 104
40 NS-opt15  0x00005ffda8b57cd0 NorthStarToFuncPassPass::runOnOperation() + 812
41 NS-opt15  0x00005ffda875f90d
42 NS-opt15  0x00005ffda8763604
43 NS-opt15  0x00005ffda876aba2
44 NS-opt15  0x00005ffda876dd97
45 NS-opt15  0x00005ffda875fd26 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) + 998
46 NS-opt15  0x00005ffda876000c mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) + 382
47 NS-opt15  0x00005ffda8761f08 mlir::PassManager::runPasses(mlir::Operation*, mlir::AnalysisManager) + 90
48 NS-opt15  0x00005ffda8761d5b mlir::PassManager::run(mlir::Operation*) + 1155
49 NS-opt15  0x00005ffda8742100
50 NS-opt15  0x00005ffda87427de
51 NS-opt15  0x00005ffda8742d35
52 NS-opt15  0x00005ffda8743e55
53 NS-opt15  0x00005ffda87e3ad7
54 NS-opt15  0x00005ffda87e3263 mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<llvm::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, llvm::StringRef, llvm::StringRef) + 169
55 NS-opt15  0x00005ffda8742eae mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::DialectRegistry&, mlir::MlirOptMainConfig const&) + 330
56 NS-opt15  0x00005ffda8743172 mlir::MlirOptMain(int, char**, llvm::StringRef, llvm::StringRef, mlir::DialectRegistry&) + 641
57 NS-opt15  0x00005ffda874336c mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&) + 322
58 NS-opt15  0x00005ffda8739442 main + 150
59 libc.so.6 0x00007fee83a29d90
60 libc.so.6 0x00007fee83a29e40 __libc_start_main + 128
61 NS-opt15  0x00005ffda87391e5 _start + 37
